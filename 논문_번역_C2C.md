# CACHE-TO-CACHE: 대형 언어 모델 간 직접 의미 통신

**Preprint 버전**

Tianyu Fu¹'², Zihan Min¹, Hanling Zhang³, Jichao Yan¹,
Guohao Dai⁵'², Wanli Ouyang³'⁴, Yu Wang†¹

¹Tsinghua University ²Infinigence AI ³The Chinese University of Hong Kong
⁴Shanghai AI Laboratory ⁵Shanghai Jiao Tong University

†교신저자: Yu Wang (yu-wang@tsinghua.edu.cn)

## 초록 (ABSTRACT)

다중 LLM 시스템은 다양한 대형 언어 모델의 상호 보완적 강점을 활용하여 단일 모델로는 달성할 수 없는 성능과 효율성 향상을 실현합니다. 기존 설계에서 LLM은 텍스트를 통해 통신하며, 이로 인해 내부 표현이 출력 토큰 시퀀스로 변환되어야 합니다. 이 과정은 풍부한 의미 정보를 손실시키고 토큰별 생성 지연을 발생시킵니다.

이러한 한계에 동기를 얻어, 우리는 다음과 같은 질문을 던집니다: **LLM이 텍스트를 넘어 통신할 수 있을까?**

오라클 실험 결과, KV-Cache의 의미를 풍부하게 하면 캐시 크기를 늘리지 않고도 응답 품질을 개선할 수 있으며, 이는 KV-Cache가 모델 간 통신을 위한 효과적인 매체임을 뒷받침합니다. 따라서 우리는 **Cache-to-Cache (C2C)**를 제안합니다. 이는 LLM 간 직접 의미 통신을 위한 새로운 패러다임입니다.

C2C는 신경망을 사용하여 소스 모델의 KV-Cache를 대상 모델의 공간으로 투영하고 융합하여 직접적인 의미 전달을 가능하게 합니다. 학습 가능한 게이팅 메커니즘이 캐시 통신으로부터 이익을 얻는 대상 레이어를 선택합니다.

텍스트 통신과 비교하여, C2C는 두 모델의 깊고 전문화된 의미를 활용하면서 명시적인 중간 텍스트 생성을 피합니다. 실험 결과, C2C는 개별 모델보다 평균 8.5-10.5% 더 높은 정확도를 달성했습니다. 또한 텍스트 통신 패러다임보다 약 3.0-5.0% 우수하면서 평균 2.0배의 지연 속도 향상을 제공합니다.

코드는 https://github.com/thu-nics/C2C 에서 확인할 수 있습니다.

---

## 1. 서론 (INTRODUCTION)

대형 언어 모델(LLM)의 급속한 발전과 함께, 이들은 점점 더 다양한 도메인과 작업에 적용되고 있습니다. 다양한 요구를 충족하기 위해, LLM은 코딩(Hui et al., 2024), 수학(Yang et al., 2024a), 시각적 이해(Bai et al., 2025), 엣지 컴퓨팅(Zhang et al., 2024b) 등 서로 다른 초점을 가지고 훈련됩니다. 한편, 범용 LLM도 프롬프트 엔지니어링을 통해 전문화된 기능을 시뮬레이션할 수 있어, 다운스트림 애플리케이션에서 유연한 역할 적응이 가능합니다.

LLM의 다양성을 활용하여, 전체 성능과 효율성을 더욱 향상시키기 위한 많은 다중 LLM 시스템이 제안되었습니다(Guo et al., 2024; Tran et al., 2025). **협업 다중 LLM 시스템**(Li et al., 2023; Wu et al., 2023)에서는 LLM에게 서로 다른 역할이 할당되고 적극적으로 텍스트 메시지를 교환합니다. 인간의 협업을 반영하여, 이러한 시스템은 언어적 의사소통을 통해 서로 다른 에이전트로부터 부분적인 이해나 하위 솔루션을 축적합니다. 이들은 여러 LLM의 집단적 능력을 활용하여 단일 모델로는 해결할 수 없는 복잡한 문제를 해결합니다.

대조적으로, **라우팅 기반 다중 LLM 추론 시스템**은 적극적인 메시지 교환보다는 수동적인 컨텍스트 상속에 의존합니다. 이러한 시스템은 더 동적이고 효율적인 응답을 위해 다양한 매개변수 크기나 추론 깊이의 모델을 조정합니다(Li et al., 2024; Fu et al., 2025; Ong et al., 2024; OpenAI, 2025). 다운스트림 모델은 다중 라운드 대화에서 이전 모델의 컨텍스트를 상속한 다음, 대화 기록에 대한 자체 이해를 바탕으로 새로운 질문에 대한 후속 응답을 생성합니다.

### 텍스트-텍스트(T2T) 통신의 한계

그러나 현재의 **텍스트-텍스트(T2T)** 인터페이스는 LLM 간의 정보 교환을 제한합니다. 특히 공유된 컨텍스트에 대한 풍부하거나 다양한 의미적 해석을 전달할 때 그렇습니다. 그림 2에 설명된 바와 같이, 이러한 한계는 T2T 통신의 여러 내재적 제약에서 발생합니다:

1. **정보 병목 현상**: 텍스트는 저대역폭 매체로서, 고차원 내부 표현을 반복적으로 선형 문자열로 압축한 다음 수신자 LLM이 다시 압축 해제해야 합니다. 모델이 지식이나 할당된 역할에서 다를 때, 일부 신호는 복구할 수 없습니다(예: `<p>`를 섹션 마커로 해석).

2. **자연어의 모호성**: 자연어는 본질적으로 모호하며, 관용구, 불특정 참조, 모호한 표현이 있습니다. 최근 에이전트 프로토콜이 텍스트 메시지를 표준화하려 하지만(Anthropic, 2024; Surapaneni et al., 2025), 경직된 템플릿은 유연한 개방형 협업에 여전히 불충분합니다.

3. **생성 지연**: T2T 통신은 눈에 띄는 지연을 발생시킵니다. 모든 교환은 순차적으로 컨텍스트 설명의 철저한 토큰별 디코딩이 필요합니다.

이러한 한계는 핵심 질문을 제기합니다:

> **LLM이 텍스트를 넘어 통신할 수 있을까?**

### KV-Cache를 통한 통신

본 연구에서는 **KV-Cache를 LLM 통신의 매체로 사용하는 것을 탐구합니다**. KV-Cache는 텍스트보다 자연스럽게 더 풍부한 표현입니다. 또한 직접 투영을 통해 완전히 병렬적인 통신을 가능하게 하여, 텍스트 교환에서의 느린 순차적 디코딩을 피합니다.

우리의 오라클 실험은 다음을 보여줍니다:
1. 동일한 컨텍스트 길이에서 KV-Cache를 풍부하게 하면 정확도가 증가할 수 있습니다.
2. KV-Cache는 LLM 간에 변환 가능합니다.
3. 서로 다른 LLM은 동일한 입력에 대해 서로 다른 의미적 이해와 컨텍스트 지식을 인코딩하며, 이는 그들의 상호 보완적 강점을 반영합니다.

이러한 오라클에 고무되어, 우리는 **Cache-to-Cache (C2C)**를 제안합니다. 이는 더 풍부하고 빠른 다중 LLM 통신을 위한 새로운 패러다임입니다.

그림 1(b)에 표시된 바와 같이, C2C는 소스 모델의 KV-Cache를 대상 모델의 공간으로 투영하고 신경 Cache Fuser를 통해 병합합니다.

### 주요 결과

실험 결과:
- C2C는 개별 모델보다 평균 **8.5-10.5% 더 높은 정확도**를 달성
- T2T 패러다임보다 약 **3.0-5.0% 우수한 성능**
- 평균 **2.0배의 지연 속도 향상**

---

## 2. 관련 연구 (RELATED WORK)

### 2.1 KV-Cache 공유 및 재사용

레이어 간 KV-Cache의 유사성을 기반으로, 단일 LLM 추론을 가속화하기 위해 얕은 레이어의 KV-Cache를 더 깊은 레이어에서 재사용하는 **모델 내 캐시 공유 방법**(Yang et al., 2024b; Wu & Tu, 2024; Sun et al., 2024; Brandon et al., 2024; Wu et al., 2025)이 제안되었습니다.

또 다른 연구 초점은 동일한 모델의 여러 사용자 쿼리에서 KV-Cache의 일부(예: 공통 접두사, 참조 문서)를 재사용하는 것입니다(Bang, 2023; Ye et al., 2024; Yao et al., 2024; Qin et al., 2024; Yang et al., 2025b). DroidSeek(Liu et al., 2024a)은 동일한 기본 모델에서 미세 조정된 모델로 캐시 재사용을 확장합니다.

**우리의 차이점**: 캐시 재사용을 통한 계산 효율성에 초점을 맞춘 기존 작업과 달리, 우리의 접근 방식은 KV-Cache를 LLM 간 **의미 전달을 위한 매체**로 활용합니다. 또한, 동일한 모델 또는 동일한 구조와 크기의 모델에만 제한된 기존 캐시 공유 방법과 달리, 우리의 방법은 **서로 다른 모델 패밀리와 다양한 모델 크기 간 공유**를 지원합니다.

### 2.2 다중 LLM 시스템

#### 협업 다중 LLM 시스템

협업 시스템은 여러 LLM을 집단 성능을 향상시키기 위해 정보를 교환하는 동료로 취급합니다:

- **Chain-of-Agents**(Zhang et al., 2024c)와 **MetaGPT**(Hong et al., 2023): 에이전트가 자연어 인터페이스를 사용하여 직접 통신하는 순차적 메시지 흐름을 생성합니다.

- **Mixture-of-Agents**(Wang et al., 2024)와 **DyLAN**(Liu et al., 2024b): 계층화된 통신 아키텍처를 도입합니다. 대상 LLM은 투표 또는 요약 메커니즘을 사용하여 여러 모델의 메시지를 집계합니다.

- **다중 에이전트 토론 방법**(Estornell & Liu, 2024; Liang et al., 2024; Du et al., 2023): 반복적인 통신 라운드를 포함하여, LLM 에이전트가 응답을 논의하고 개선하도록 합니다.

- **MCP**(Anthropic, 2024)와 **A2A**(Surapaneni et al., 2025)와 같은 최근 작업: 자연어를 넘어 공식 텍스트 프로토콜을 확립하여, 협업 다중 LLM 시스템에서 에이전트 상호 작용 및 도구 사용을 표준화합니다.

이러한 접근 방식은 텍스트 수준 인터페이스에 의존하며, 통신에는 한 모델이 토큰별로 텍스트를 생성하고 다른 모델이 이를 입력으로 수집해야 합니다. **우리의 작업은 내부 KV-Cache 표현을 직접 공유함으로써 더 깊고 효율적인 협업을 탐구합니다.**

#### 라우팅 기반 다중 LLM 추론 시스템

LLM 추론을 가속화하기 위해, 여러 시스템이 서로 다른 능력과 비용을 가진 여러 모델을 활용합니다:

- **동적 모델 선택 방법**(OpenAI, 2025; Ong et al., 2024; Feng et al., 2024): 효율성과 성능의 균형을 맞추기 위해 쿼리를 서로 다른 크기와 구성의 모델로 라우팅합니다.

- **토큰 수준 라우팅 방법**(Zhang et al., 2024a; Shen et al., 2024; Zheng et al., 2025; Fu et al., 2025): 더 세밀한 선택을 가능하게 하며, 복잡한 작업의 추론 프로세스 내에서 간단한 토큰 생성에 더 작은 모델을 활용합니다.

이러한 시스템은 전략적 모델 전환을 통해 효율성을 달성하지만, 다른 모델의 컨텍스트를 완전히 삭제하거나 단순히 컨텍스트에 대한 자체 이해에 의존합니다. **이해 공유가 없으면, 더 작은 모델은 더 큰 모델이 이미 계산한 더 풍부한 표현으로부터 이익을 얻을 수 없습니다.**

---

## 3. 방법론 (METHOD)

### 3.1 예비 사항 (PRELIMINARIES)

#### LLM 추론

자동 회귀 LLM 추론은 두 단계로 구성됩니다: **prefill**과 **decode**.

- **Prefill**: 전체 입력을 인코딩하여 첫 번째 출력 토큰을 생성
- **Decode**: 마지막 토큰과 캐시된 key-value (KV) 상태를 사용하여 이후 토큰을 반복적으로 생성

공식적으로, 입력 토큰 시퀀스를 X[0:n] = [x₀, ..., xₙ₋₁]이라고 하겠습니다. prefill 후, LLM은 토큰당 KV-Cache를 생성합니다:

**C(X[0:n]) = [c₀, ..., cₙ₋₁] ∈ ℝⁿˣᵈ**

여기서 d는 모든 레이어에서 단일 벡터로 평면화된 KV 차원을 나타냅니다.

디코딩 중에, 현재 토큰 yᵢ와 입력 및 생성된 접두사의 캐시를 사용하여 다음 토큰이 예측됩니다:

**yᵢ₊₁ = P(yᵢ | C(X) ⊕ C(Y[0:i]))**

여기서 ⊕는 시퀀스별 연결을 나타냅니다. 캐시는 C(Y[0:i+1]) = C(Y[0:i]) ⊕ C(yᵢ)로 업데이트됩니다.

#### LLM 통신

LLM 통신 시나리오에서:
- **Sharer**: 컨텍스트 이해 또는 지식을 제공하는 LLM
- **Receiver**: 이를 활용하는 LLM

### 3.2 Cache-to-Cache 통신을 위한 오라클 (ORACLES)

우리는 LLM이 KV-Cache를 통해 직접 의미 통신을 할 수 있는지 탐구하고자 합니다. 구체적으로, 다음 질문에 답하기 위해 두 가지 오라클 실험을 설계했습니다:

1. **이점**: 시퀀스 길이를 늘리지 않고 KV-Cache 의미 풍부화를 통해 모델의 능력을 향상시킬 수 있는가?
2. **변환 가능성**: 한 모델의 KV-Cache를 다른 모델이 효과적으로 활용할 수 있는가?

#### 3.2.1 캐시 풍부화 오라클 (CACHE ENRICHMENT ORACLE)

캐시 풍부화의 이점을 검증하기 위해, 먼저 고정 길이 질문 KV-Cache의 의미적 품질을 캐시 크기를 늘리지 않고 개선할 수 있는지 탐구합니다.

Few-shot 프롬프팅은 이것이 작동할 수 있음을 시사합니다: 질문 X 전에 예제 E를 제공하면 종종 정확도가 향상됩니다. 그러나 이것이 더 많은 컨텍스트 토큰에 주의를 기울이는 것에서 발생하는지, 아니면 E가 KV-Cache에서 X가 임베딩되는 방식을 풍부하게 하는 것에서 발생하는지?

세 가지 설정을 통해 평가합니다:

1. **Direct**: X에만 prefill하고 C(X)로 디코딩
2. **Few-shot**: E⊕X에 prefill하고 C(E⊕X)로 디코딩 (더 긴 캐시)
3. **Oracle**: E⊕X에 prefill하지만 예제 세그먼트를 삭제하고 질문에 정렬된 슬라이스만 유지:

   **C*(X) = C[|E|:|E|+|X|](E ⊕ X)**

   디코딩은 추가 토큰 없이 질문 길이 캐시를 사용합니다.

표 1에 표시된 바와 같이, Oracle 설정은 동일한 캐시 길이에서 응답 품질을 향상시킵니다.

**표 1: 캐시 풍부화 실험**

| Method | Cache Len. | Cache Augment | Acc. (%) |
|--------|-----------|---------------|----------|
| Direct | \|X\| | No | 58.42 |
| Few-shot | \|E\| + \|X\| | Yes | 63.39 |
| Oracle | \|X\| | Yes | 62.34 |

또한, 캐시 풍부화가 서로 다른 트랜스포머 레이어에 어떻게 영향을 미치는지 분석했습니다. 우리의 발견은 레이어 전반에 걸쳐 상당한 변동을 보여줍니다: 일부 레이어는 캐시 풍부화로부터 이익을 얻지만, 다른 레이어는 성능 저하를 경험합니다(부록 A.2.1 참조).

그림 4에 표시된 바와 같이, 성능이 가장 좋은 상위 10개 레이어에 선택적으로 캐시 풍부화를 적용하면 모든 레이어를 풍부하게 하는 것보다 더 높은 정확도를 얻을 수 있으며, 성능이 가장 나쁜 레이어를 대상으로 하면 정확도가 감소합니다. 이 발견은 우리의 cache Fuser의 게이팅 메커니즘을 안내합니다(섹션 3.3.2).

#### 3.2.2 캐시 변환 오라클 (CACHE TRANSFORMATION ORACLE)

한 모델의 KV-Cache를 다른 모델이 활용할 수 있는지 검증하기 위해, 교차 모델 변환 실험을 수행했습니다. 소스 LLM(Qwen3-4B)의 KV-Cache를 대상 LLM(Qwen3-0.6B)으로 매핑하기 위해 3계층 MLP를 훈련했습니다(추가 설정은 부록 A.3.2 참조).

그림 3의 T-SNE 시각화는 두 LLM의 원시 KV-Cache가 표현 공간에서 멀리 떨어져 있음을 보여줍니다. 변환 후, 매핑된 KV-Cache는 대상 모델의 KV-Cache 표현 공간 내부에 있습니다. 이러한 결과는 서로 다른 모델의 KV-Cache가 일반적으로 변환 가능함을 입증합니다.

**표 2: Sharer, Receiver 및 C2C 융합된 KV-Cache의 평균 유효 순위**

| Type | Sharer | Receiver | C2C |
|------|--------|----------|-----|
| K Cache | 539 | 388 | 395 |
| V Cache | 689 | 532 | 560 |

주목할 점은 변환된 캐시가 대상 공간의 더 작은 부분 집합만 차지한다는 것입니다. 이것은 소스 모델의 의미 정보가 소스가 더 크더라도 대상의 정보를 완전히 커버할 수 없음을 나타냅니다. 이는 각 모델이 컨텍스트를 인코딩하는 방식의 본질적인 차이를 반영합니다.

또 다른 관찰도 이 해석을 뒷받침합니다: 서로 다른 모델의 정답 세트는 제한된 중첩을 보입니다(그림 7), 각 모델의 집계된 정확도가 비슷함에도 불구하고. 이러한 발견은 서로 다른 모델의 전문화된 컨텍스트 이해가 성공적으로 투영되고 융합될 수 있다면, 각 모델의 상호 보완적 강점을 활용할 수 있음을 시사합니다.

### 3.3 C2C 설계

#### 3.3.1 개요

오라클 실험을 바탕으로, C2C Fuser 아키텍처를 제안합니다. 그 핵심 목표는 한 모델(Sharer)에서 유용한 컨텍스트 이해 또는 지식을 추출하여 다른 모델(Receiver)로 융합하는 것입니다.

일반적으로, C2C 패러다임은 일련의 key/value 캐시 융합기(Fuser) ℱ와 레이어 매핑 전략 𝒢를 포함합니다. prefill 단계 동안, 융합기 ℱₙ은 Receiver 모델의 n번째 레이어 캐시 Cₙ(X)와 Sharer 모델의 해당 𝒢(n)번째 레이어 캐시 C^S_{𝒢(n)}(X)를 받아 융합된 캐시를 생성합니다:

**𝒞^F = {ℱₙ(Cₙ(X), C^S_{𝒢(n)}(X))}^N_{n=1}**

디코딩 중에, 현재 토큰 yᵢ와 입력 및 생성된 접두사의 캐시를 사용하여 다음 토큰이 예측됩니다:

**yᵢ₊₁ = P(yᵢ | 𝒞^F(X) ⊕ C(Y[0:i]))**

#### 3.3.2 Fuser 구조

Receiver의 KV-Cache를 파괴적으로 덮어쓰지 않고 향상시키기 위해, Fuser는 **잔차 통합 원리**에 따라 설계되었습니다. 그림 5에 표시된 바와 같이, 세 가지 핵심 모듈을 포함합니다:

1. **투영 모듈 (Projection module)**: Receiver의 KV-Cache를 Sharer의 KV-Cache와 연결한 다음, 연결된 특징을 투영 레이어와 특징 융합 레이어를 통해 처리합니다.

2. **동적 가중치 모듈 (Dynamic weighting module)**: 투영된 정보를 동적으로 재가중하기 위해 입력 인식 헤드 변조 레이어를 적용합니다.

3. **학습 가능한 게이트 (Learnable gate)**: Sharer의 컨텍스트를 주입할지 결정하는 훈련 가능한 레이어별 게이트 값을 도입합니다. 이 값은 훈련 중에는 미분 가능하고 추론 시에는 이진으로 부드럽게 전환하기 위해 온도 어닐링이 있는 Gumbel-sigmoid를 적용합니다.

#### 3.3.3 모델 정렬 (MODEL ALIGNMENT)

모델 패밀리와 크기에 걸쳐 KV-Cache를 융합하려면 두 수준에서 정렬이 필요합니다: **토큰**과 **레이어**.

- **토큰 정렬**: 서로 다른 토크나이저는 동일한 입력에 대해 약간 다른 토큰 시퀀스를 생성할 수 있습니다. 각 Receiver 토큰을 문자열 형식으로 디코딩하고 Sharer의 토크나이저를 사용하여 다시 인코딩하여 정렬합니다. 일대다 매핑이 가끔 발생하면, 정보를 보존하기 위해 최대 문자열 커버리지를 가진 Sharer 토큰을 선택합니다.

- **레이어 정렬**: **터미널 정렬 전략**을 채택합니다: 두 모델의 최종 레이어가 먼저 정렬되고, 그 다음 끝에서 두 번째 레이어, 더 얕은 모델의 첫 번째 레이어에 도달할 때까지 역순으로 정렬됩니다. 자세한 사양은 부록 A.1에 제공됩니다.

#### 3.3.4 훈련 방식

훈련 중에, Sharer와 Receiver 모델을 모두 고정하고, KV-Cache 융합을 위한 C2C 모듈만 훈련합니다. Receiver의 응답 예측에 대한 표준 다음 토큰 예측 손실을 사용하며, 이는 감독된 미세 조정(SFT)과 유사합니다. 주요 차이점은 Receiver가 자체 캐시가 아닌 융합된 KV-Cache를 조건으로 응답을 예측한다는 것입니다.

훈련 절차는 세 단계로 구성됩니다:
1. **Forward**: 두 모델 모두 입력 컨텍스트를 인코딩하여 각자의 KV-Cache를 생성합니다.
2. **Fusion**: C2C 모듈이 두 KV-Cache를 융합하고 Receiver의 캐시를 교체합니다.
3. **Supervision**: Receiver가 융합된 캐시를 사용하여 응답을 prefill하고, 그래디언트는 C2C를 통해 역전파되어 예측 손실을 최소화합니다.

---

## 4. 실험 (EXPERIMENT)

### 4.1 설정

주요 설정을 여기에 강조하며, 자세한 내용은 부록 A.3에 있습니다.

#### 모델

Qwen2.5(Yang et al., 2024a; Hui et al., 2024), Qwen3(Yang et al., 2025a), Llama3.2(Dubey et al., 2024), Gemma3(Team et al., 2025)를 포함한 다양한 모델 패밀리에서 C2C를 평가합니다.

일반화 가능성을 테스트하기 위해, Sharer-Receiver 모델 조합에 대해 다음과 같은 서로 다른 구성을 선택합니다:
- 서로 다른 세대의 모델 (Qwen3 및 Qwen2.5)
- 서로 다른 패밀리 (Qwen, Llama, Gemma)
- 서로 다른 크기 (0.6B에서 14B)
- 서로 다른 전문화 (일반, 코드, 수학 모델)
- 서로 다른 훈련 단계 (사전 훈련 및 지시 미세 조정 모델)

#### 베이스라인

C2C를 두 가지 LLM 협업 방법과 비교하여 성능을 맥락화합니다:

1. **Text-to-Text (T2T) 통신**: 각 쿼리에 대한 분석-응답 전달을 통해 협업합니다. Sharer는 입력 질문을 해결하기 위한 핵심 정보의 분석 텍스트를 생성합니다. 이 텍스트는 원래 질문과 연결되어 Receiver에 제공되어 표준 협업 파이프라인을 반영합니다. 해당 프롬프트는 부록 A.3.6에 있습니다.

2. **쿼리 수준 라우팅**(Ong et al., 2024): 서로 다른 쿼리에 적절한 LLM을 선택하여 협업합니다.

또한 협업 이득을 위한 하한을 설정하기 위해 개별 모델 성능(Sharer 또는 Receiver 단독)을 포함합니다.

#### 벤치마크

다음 네 가지 널리 사용되는 벤치마크에서 평가하여 포괄적인 커버리지를 보장합니다:
- **OpenBookQA**(Mihaylov et al., 2018): 사실 기반 추론
- **MMLU-Redux**(Gema et al., 2025): 일반 도메인 지식
- **ARC-Challenge (ARC-C)**(Clark et al., 2018): 과학적 및 논리적 추론
- **C-Eval**(Huang et al., 2023): 중국어 도메인의 포괄적 지식

#### 훈련 데이터셋

C2C의 일반화 가능성을 보장하기 위해, 일반 미세 조정 데이터셋인 **OpenHermes2.5 Dataset**(Teknium, 2023)의 처음 500k 샘플을 사용하여 C2C Fuser를 훈련합니다. 훈련 예산을 위해, 스케일링 동작 및 동작 분석에서는 MMLU를 훈련 세트로 사용합니다(명시되지 않는 한).

#### 평가 설정

- **성능 지표**: 평균 정확도
- **평가 모드**: 텍스트 생성 및 답변 추출
- **최대 생성 길이**: 다지선다 벤치마크의 경우 64
- **설정**: 재현성을 보장하기 위해 zero-shot 설정과 zero generation temperature
- **효율성 지표**: 평균 추론 시간 (배치 크기 = 1인 단일 NVIDIA A100 GPU 사용)

### 4.2 성능 및 효율성

표 8에 표시된 바와 같이, C2C는 서로 다른 설정과 벤치마크 전반에 걸쳐 Receiver 모델 성능을 일관되게 향상시킵니다.

**주요 결과:**
- C2C 적용 후, 세 가지 다른 Sharer에 걸쳐 평균 정확도가 각각 **11.00%, 9.64%, 11.88%** 증가
- Text-to-text 통신과 비교하여, C2C는 평균 정확도가 **5.36%, 4.15%, 3.06%** 증가
- 중간 텍스트 메시지 생성을 면제하여 **3.46×, 1.51×, 14.41×**의 명백한 속도 향상 달성
- 대조적으로, 쿼리 수준 라우팅은 효율성을 우선시하지만 정확도를 두 원래 모델 중 더 나은 것으로 제한

**표 3: 벤치마크 전반의 통신 방법 비교 (Receiver: Qwen3-0.6B)**

예를 들어, Qwen2.5-0.5B를 Sharer로 사용할 때:

| Task | Metric | Receiver | Sharer | Routing | T2T | C2C |
|------|--------|----------|--------|---------|-----|-----|
| MMLU-Redux | Acc | 35.53 | 38.42 | 35.58 | 41.03 | **42.92** |
| | Time | 0.29 | 0.34 | 0.27 | 1.52 | **0.40** |
| OpenBook | Acc | 39.20 | 45.60 | 40.80 | 44.00 | **52.60** |
| | Time | 0.27 | 0.35 | 0.29 | 0.81 | **0.30** |
| ARC-C | Acc | 41.04 | 42.09 | 40.70 | 49.48 | **54.52** |
| | Time | 0.29 | 0.39 | 0.29 | 1.00 | **0.36** |
| C-Eval | Acc | 32.04 | 40.21 | 34.61 | 35.88 | **41.77** |
| | Time | 0.26 | 0.31 | 0.26 | 1.51 | **0.34** |

특히, Qwen3-4B Base를 Sharer로 사용할 때, 생성된 텍스트가 때때로 지시를 무시하고 예상 길이를 초과합니다. 이로 인해 text-to-text 통신 시간이 극도로 길어지는 반면, C2C는 이 문제를 우회합니다. 이 설정은 C2C의 흥미로운 사용 사례를 강조합니다: **약한 SFT 모델이 강한 사전 훈련된 기본 모델이 지시를 따르도록 할 수 있습니다.**

### 4.3 스케일링 동작

#### 시퀀스 길이 스케일링

LongBenchV1 벤치마크의 긴 컨텍스트 작업에서 C2C가 시퀀스 길이와 관련하여 어떻게 스케일링되는지 평가합니다. 모든 C2C 융합기는 LongBenchV1의 서로 다른 세트에서 훈련 및 테스트됩니다.

**표 4: 입력 길이에 걸친 LongBenchV1 점수 (Qwen3-0.6B (Receiver) 및 Qwen2.5-0.5B (Sharer))**

| Length | Receiver | Sharer | T2T | C2C |
|--------|----------|--------|-----|-----|
| 0-4k | 27.39 | 21.89 | 29.47 | **36.64** |
| 4-8k | 24.97 | 18.55 | 26.30 | **31.71** |
| 8k+ | 22.20 | 14.04 | 24.54 | **25.37** |

표 4에 표시된 바와 같이, C2C는 모든 시퀀스 길이 구간에서 text-to-text 통신을 일관되게 능가합니다. 이는 입력 길이 범위 전반에 걸친 C2C의 이점을 나타냅니다.

#### 모델 크기 스케일링

Sharer 및 Receiver 모델 크기와 관련하여 C2C가 어떻게 스케일링되는지 조사합니다. 모든 C2C 융합기는 MMLU의 보조 훈련 분할에서 훈련되고 MMLU-Redux에서 평가됩니다.

그림 6에 표시된 바와 같이:
- x축은 Sharer 크기(Qwen2.5-Instruct 시리즈)를 나타냅니다
- y축은 Receiver 전용 베이스라인에 대한 C2C의 정확도 향상(Δ Accuracy)을 보여줍니다
- 각 곡선은 Qwen3 시리즈의 Receiver를 나타냅니다

**주요 발견:**
- C2C의 정확도 향상은 일반적으로 T2T보다 빠르게 증가합니다
- 이 추세는 Sharer가 더 풍부한 지식을 보유할 때, C2C가 Receiver에게 유용한 정보를 더 효과적으로 전달할 수 있음을 보여줍니다
- 더 큰 Receiver의 경우 상대적 이득이 덜 두드러지는데, 이는 더 강한 베이스라인과 Sharer의 지식과 더 높은 중첩 때문입니다

#### 다른 모델 조합

서로 다른 모델 패밀리 및 서로 다른 작업별 모델을 포함한 다양한 Sharer-Receiver 조합을 테스트합니다.

**표 6: Receiver 전용, Sharer 전용, T2T 및 C2C의 정확도 및 시간 비교**

예시 결과 (이기종 설정):

| Receiver | Sharer | Metric | Receiver | Sharer | T2T | C2C |
|----------|--------|--------|----------|--------|-----|-----|
| Qwen3-0.6B | Gemma3-1B | Acc | 39.20 | 31.75 | 41.35 | **45.90** |
| | | Time | 0.27 | 0.54 | 1.04 | **0.30** |
| Qwen3-0.6B | Qwen2.5-Math-1.5B | Acc | 39.20 | 39.86 | 43.71 | **46.13** |
| | | Time | 0.27 | 8.71 | 6.60 | **0.27** |
| Qwen3-0.6B | Qwen2.5-Coder-0.5B | Acc | 39.20 | 25.09 | 39.74 | **46.89** |
| | | Time | 0.27 | 0.26 | 1.59 | **0.27** |

표 6의 결과는 C2C가 5개의 모든 조합에서 text-to-text 통신을 능가하며 평균 **8.59%** 증가함을 보여줍니다. 이는 C2C를 사용함으로써 Receiver 모델이 서로 다른 모델의 컨텍스트 이해를 효과적으로 활용하여 성능을 향상시킬 수 있음을 뒷받침합니다.

C2C의 일반화 가능성을 더욱 테스트하기 위해, Sharer와 Receiver 모델을 교환합니다. 결과는 C2C가 견고하게 **5.05%**의 정확도 증가를 가져오는 반면, T2T를 적용하면 성능이 **6.3%** 감소함을 보여줍니다.

**결론:** 이러한 실험들은 C2C가 효과적이고 효율적인 새로운 LLM 통신 패러다임으로서의 확장성을 뒷받침합니다.

### 4.4 절제 연구 (ABLATION STUDY)

#### 개선의 출처

표 5에서, Receiver(Qwen3-0.6B)를 고정하고 Sharer를 변경하여 C2C 성능 향상의 출처를 절제합니다:

**표 5: 서로 다른 훈련 설정의 성능 비교**

| Setting | #Param. | OpenBook | ARC-C | MMLU | C-Eval |
|---------|---------|----------|--------|------|--------|
| Single | 596M | 45.80 | 47.65 | 36.81 | 35.81 |
| Identical | 529M | 50.60 | 52.52 | 42.17 | 40.34 |
| C2C | 478M | **52.60** | **54.52** | **42.92** | **41.77** |

- **Single**: Sharer 없이 Receiver의 표준 전체 미세 조정
- **Identical**: Sharer와 Receiver가 모두 Qwen3-0.6B인 C2C
- **C2C**: 기본 설정 (Sharer로 Qwen2.5-0.5B 사용)

동일한 훈련 구성에서, C2C는 Single 및 Identical보다 일관되게 더 높은 정확도를 달성합니다. 이는 C2C 개선이 순전히 추가된 훈련 가능한 용량이나 훈련 세트에 대한 과적합에서 비롯되지 않음을 확인합니다. 대신, 이기종 Sharer가 기여하는 상호 보완적인 컨텍스트 이해를 가리킵니다.

Identical이 여전히 Single을 능가하는 것은 캐시 수준 자체 통신이 유용한 보조 이해를 제공할 수 있음을 나타내며, 이는 잠재 추론 및 순환 트랜스포머에서 관찰된 효과를 반영합니다(Zeng et al., 2025; Saunshi et al., 2025).

#### Fuser 아키텍처

표 7에서 C2C 설계의 서로 다른 구성 요소의 효과를 보여줍니다:

**표 7: MMLU, ARC-C, OpenBook 및 CEval 벤치마크의 성능 비교**

| Method | MMLU | ARC-C | OpenBook | CEval | Average |
|--------|------|-------|----------|--------|---------|
| Project | 20.01 | 19.57 | 21.80 | 21.41 | 20.70 |
| +Fuse | 43.36 | 51.65 | 47.60 | 36.91 | 44.88 |
| +Gate (=C2C) | **42.92** | **54.52** | **52.60** | **41.77** | **47.95** |

- 순수 투영과 비교하여, Receiver의 원래 KV-Cache를 유지하고 Sharer의 KV-Cache와 융합하면 두 모델의 능력을 결합하여 정확도가 **24.18%** 증가합니다
- 융합된 레이어 선택을 위한 게이트를 추가하면 평균 정확도가 **3.07%** 더 증가합니다

### 4.5 동작 분석

#### 유효 순위 분석 (Effective Rank Analysis)

cache-to-cache 통신 전후의 KV-Cache의 유효 순위를 분석합니다. 유효 순위(Roy & Vetterli, 2007)는 모델 가중치 또는 활성화 값의 고유 차원을 측정하는 일반적인 접근 방식입니다. 더 높은 고유 차원은 더 풍부한 의미 정보를 의미합니다(부록 A.4.1에 공식화).

표 2에 표시된 바와 같이:
- cache-to-cache 융합 후, K와 V의 유효 순위가 각각 388에서 395로, 532에서 560으로 증가했습니다
- 이는 C2C가 Sharer 모델의 지식을 성공적으로 변환하고 Receiver 모델에 주입하여 의미 공간을 풍부하게 함을 나타냅니다

#### 점진적 동작 (Progressive Behavior)

C2C에 의해 업데이트되는 컨텍스트 KV-Cache의 비율을 점진적으로 증가시켜 C2C의 점진적 동작을 분석합니다. 비율이 50%를 초과하면, 비율을 계속 증가시키면 더 나은 성능을 얻습니다. 자세한 설정 및 분석은 부록 A.2.4에서 확인할 수 있습니다.

#### 게이트 동작 (Gate Behavior)

서로 다른 훈련 체제에서 C2C의 학습 가능한 게이트의 동작을 분석합니다(부록 A.4.2에 자세히 설명).

---

## 5. 향후 연구 (FUTURE WORK)

일반적인 LLM 통신 패러다임으로서, C2C는 다양한 분야로 확장될 수 있습니다. 몇 가지 잠재적 시나리오는 다음과 같습니다:

1. **개인 정보 보호 인식 클라우드-엣지 협업**: 클라우드 규모 모델이 큐레이션된 KV-Cache 세그먼트를 엣지 모델에 전송하여 원시 텍스트를 방출하지 않고 기능을 향상시킬 수 있으며, 대역폭을 줄이고 콘텐츠 노출을 제한합니다.

2. **현재 추론 가속 방법과의 통합**: C2C를 사용하여 투기적 디코딩을 향상시키고 낮은 지연 및 비용을 위해 이기종 모델 간 토큰 수준 라우팅을 가능하게 합니다.

3. **다중 모달 통합**: 언어 추론 LLM, 비전-언어 모델(VLM) 및 비전-언어-행동(VLA) 정책 간에 캐시를 정렬하고 융합하여 언어 및 시각적 컨텍스트가 더 정확한 행동을 유도할 수 있도록 합니다.

---

## 6. 결론 (CONCLUSION)

결론적으로, 우리는 **LLM이 텍스트를 넘어 통신할 수 있음**을 입증합니다. 우리는 모델 간에 key-value (KV) 캐시를 변환하고 융합하여 직접적인 의미 통신을 가능하게 하는 일반적인 패러다임인 **Cache-to-Cache (C2C)**를 소개합니다.

다양한 작업 및 모델 구성에 걸쳐, C2C는 일관되게 text-to-text 통신보다 더 높은 작업 성능과 더 나은 효율성을 달성합니다. 이러한 결과는 cache-to-cache를 토큰 기반 통신에 대한 실용적인 대안으로 확립하고, 확장 가능하고 낮은 지연의 다중 LLM 시스템에 대한 약속을 강조합니다.

---

## 참고문헌 (주요 참고문헌)

논문의 전체 참고문헌 목록은 원문 11-13페이지에서 확인할 수 있습니다.

---

## 부록 (APPENDIX)

논문의 부록(14-23페이지)에는 다음과 같은 추가 내용이 포함되어 있습니다:

### A.1 설계 선택 탐색
- 레이어 정렬 전략
- 토크나이제이션 정렬
- Fuser 아키텍처

### A.2 추가 실험 결과
- 캐시 풍부화 세부 사항
- Strong-to-Weak 통신
- 정확도 분석
- 점진적 동작

### A.3 추가 실험 설정
- 캐시 풍부화
- 캐시 변환
- 쿼리 수준 라우팅
- 평가 방법
- C2C 훈련
- 평가 프롬프트

### A.4 추가 분석
- 유효 순위
- 게이팅 동작
- 추론 시간의 이상치 사례
- 예시 모델 출력

---

**번역 완료일**: 2025년 1월

**원본 논문**: arXiv:2510.03215v1 [cs.CL] 3 Oct 2025

**GitHub**: https://github.com/thu-nics/C2C
